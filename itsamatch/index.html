<!DOCTYPE html>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
    
<script type="text/javascript"
        src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG">
</script>

<html>
<head>
  <meta charset="utf-8">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Vision-language models, Multimodal learning, Quadratic assignment problem, Representation learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>It's a (Blind) Match! Towards Vision-Language Correspondence without Parallel Data</title>
  <link rel="icon" type="image/x-icon" href="static/images/icon.png">
  <link href="https://fonts.cdnfonts.com/css/grandview" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- <img src="./static/videos/itsablindmatch_title.gif" width="33%"> -->
            <video id="title" autoplay muted playsinline width="33%">
              <source src="./static/videos/itsablindmatch_title.mov"
                      type="video/mp4">
              <source src="./static/videos/itsablindmatch_title.mp4"
              type="video/mp4">
            </video>
            <h1 class="title is-1 publication-title">Towards Vision-Language Correspondence without Parallel Data</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://dominik-schnaus.github.io/" target="_blank">Dominik Schnaus</a>,</span>
                <span class="author-block">
                  <a href="https://arnike.github.io/" target="_blank">Nikita Araslanov</a><sup>&dagger;</sup>,</span>
                  <span class="author-block">
                    <a href="https://cvg.cit.tum.de/members/cremers" target="_blank">Daniel Cremers</a><sup>&dagger;</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>&dagger;</sup>equal advising<br>
                      <!-- <div class="slot-machine">
                          <div class="reel" id="reel">
                              <div>TU Munich</div>
                              <div class="icon"><img src="./static/images/tum_logo.png"></div>
                          </div>
                      </div> -->
                      TU Munich, Munich Center of Machine Learning
                      <br>CVPR 2025</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2503.24129"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- ArXiv abstract Link -->
                    <span class="link-block">
                      <a href="https://arxiv.org/abs/2503.24129"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <span>arXiv</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/dominik-schnaus/itsamatch"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The platonic representation hypothesis suggests that vision and language embeddings become more homogeneous as model and dataset sizes increase. In particular, pairwise distances within each modality become more similar. This suggests that as foundation models mature, <b>it may become possible to match vision and language embeddings in a fully unsupervised fashion</b>, i.e., without parallel data. We present the first study towards this prospect, and investigate conformity of existing vision and language foundation models in the context of "blind" matching. First, <b>we formulate unsupervised matching as a quadratic assignment problem</b> and <b>introduce a novel heuristic that outperforms previous solvers</b>. We also develop a technique to find optimal matching problems, for which a non-trivial match is very likely. Second, we conduct an <b>extensive study deploying a range of vision and language models on four datasets</b>. Our analysis reveals that for many problem instances, vision and language representations <b>can be indeed matched without supervision</b>. This finding opens possibility for exciting applications embedding semantic knowledge into other modalities. As a showcase, we demonstrate a proof-of-concept unsupervised classifier, which achieves non-trivial classification accuracy without any image-text annotation.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Teaser figure-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted controls loop playsinline width="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
        <source src="./static/videos/teaser.mov"
                type="video/mp4">
      </video>
    </div>
  </div>
</section>
<!-- End teaser figure -->


<!-- Paper overview -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Overview</h2>
    <div class="content has-text-justified">
      <p>
        <b>Vision-Language models need a lot of paired training data. Can we match vision and language without any supervision? Our work shows that it could be indeed feasible.</b>
        <br>
        Overall, our contributions are:
        <ul>
          <li>The formulation of <b>vision-language matching as a quadratic assignment problem (QAP)</b></li>
          <li>A specialized <b>memory-efficient QAP solver</b></li>
          <li>An <b>extensive study</b> showing the feasibility of non-trivial vision-language matching without any parallel data</li>
        </ul>

      </p>
    </div>
  </div>
</section>
<!-- End paper overview -->


<!-- Platonic representation hypothesis (+ link) without figure -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">The platonic representation hypothesis</h2>
    <div class="content has-text-justified">
      <p>
        The main motivation for our approach comes from the <a href="https://phillipi.github.io/prh/">platonic representation hypothesis</a>. The hypothesis postulates that <b>pairwise similarities in self-supervised vision and language models become closer by scaling model and dataset size</b>. In our example, for both large language and vision models trained on enough data, the cat is closer to the dog than to the airplane in the embedding space. 
      </p>
    </div>
  </div>
</section>
<!-- End platonic representation hypothesis -->

<!-- Is matched data more similar? -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Is matched data more similar?</h2>
    <div class="columns is-centered">
      <div class="column">
        <div class="content">
          <p class="content has-text-justified">
            First, we investigate how the alignment measures change when the ground-truth pairing is distorted. That is, we shuffle a subset of the data points in one modality and compute the alignment. We consider the Mutual k-NN, centered kernel alignment (CKA), and the Gromov-Wasserstein distance on several classification and image/caption datasets. We observe that <b>the average alignment decreases strictly monotonically so that the ground-truth pairing achieves the optimal alignment</b>. This shows that these similarity measures are practical heuristics for finding a matching.
            <br>
            Interestingly, we observed the same qualitative behavior for randomly initialized vision and language networks. We analyze and discuss this in more detail in Appendix C of our paper.
          </p>
        </div>
      </div>
      <div class="column">
        <div class="columns is-centered">
          <div class="column content">
            <img src="static/images/alignment.png" alt="alignment"/>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Blind matching. -->
<section class="section">
  <div class="container is-max-desktop">
  <h2 class="title is-3">Blind matching</h2>
    <div class="content">
      <p class="content has-text-justified">
        Since the alignment tends to be better for correctly paired representations, we <b>formulate the matching as an optimization problem</b>, where we search for the optimal permutation matrix $\mathbf{P}^*$ such that the alignment $l(\mathbf{X}_{ik}, \mathbf{Y}_{jl})$ is maximal (or the distance is minimal for the Gromov-Wasserstein distance):
        \(
          \DeclareMathOperator*{\argmin}{arg\,min}
          \DeclareMathOperator*{\argmax}{arg\,max}
        \)
        $$
          \mathbf{P}^* \in \argmin_{\mathbf{P} \in \mathcal{P}_N} \quad \sum_{i, j, k, l = 1}^{N} l\left(\mathbf{X}_{i k}, \mathbf{Y}_{j l}\right) \mathbf{P}_{i j} \mathbf{P}_{k l}
        $$
        $$
          \mathcal{P}_N = \{\mathbf{P} \mid \mathbf{P} \in \{0, 1\}^{N \times N}, \mathbf{P} \mathbf{1} = \mathbf{1}, \mathbf{P}^T \mathbf{1} = \mathbf{1}\}.
        $$
        Here, $\mathbf{X}_{ik}$ is the similarity of the i-th and k-th vision embeddings and accordingly $\mathbf{Y}$ is the pairwise similarity matrix in the language space. Subject to a specific instantiation of  $l(\cdot, \cdot)$, this formulation is general and can accommodate many existing distance (and similarity) measures. This problem is a quadratic assignment problem (QAP), which is in general NP-hard. Existing solvers already fail for $N \geq 30$, meaning that a 30-class problem cannot be solved.
        <br>
        We extend the <b>Hahn-Grant</b> heuristic to solve the QAP with the following <b>key extensions</b>:
        <ul>
          <li>A <b>primal heuristic</b> to find primal solutions</li>
          <li>Improved <b>memory complexity</b> from $O(N^4)$ to $O(N^3)$</li>
          <li><b>Faster</b> linear assignment solver.</li>
        </ul>
        With this solver, we find better primal solutions with tighter bounds also for larger problem sizes (optimal up to $N = 40$).
      </p>
      <center>
        <video autoplay muted controls loop playsinline width="100%">
          <source src="./static/videos/matching.mov"
                  type="video/mp4">
          <source src="./static/videos/matching.mp4"
                  type="video/mp4">
        </video>
      </center>
    </div>
  </div>
</section>
  <!--/ End blind matching. -->

<!-- Experiments. -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Experiments</h2>
    <p class="content has-text-justified">
      We evaluate how many classes can be matched correctly (=matching accuracy) in the small-scale ($N=10$) and larger-scale ($10 \leq N \leq 100$) setting.
    </p>
    <h2 class="title is-4">Small-scale matching</h2>
    <div class="content">
      <p class="content has-text-justified">
        We evaluate a total of 33 vision models and 27 language models on CIFAR-10 and CINIC-10 (we keep the image resolution from ImageNet). We observe that <b>most models perform better than 10% accuracy</b>, which is the baseline for random permutations. Furthermore, pre-training seems to be more important than model size.
        <br>
        We also find that the Gromov-Wasserstein distance is more useful for matching than Mutual k-NN and CKA.
      </p>
      <center>
        <img src="static/images/small_scale.png" alt="small-scale" width="85%"/>
      </center>
    </div>
  </div>
  <div class="container is-max-desktop">
    <h2 class="title is-4">Larger-scale matching</h2>
    <div class="content">
      <p class="content has-text-justified">
        We find that some classes are represented very differently in vision and language models. Therefore, we introduce a method to find subsets of classes that can be matched well. To do this, we optimize for a subset $\mathbf{s}^*$ of classes such that their alignment (=how similar their pairwise similarities are) is maximized:
        $$
          \mathbf{s}^* \in \argmax_{\mathbf{s}} \sum_{i, j = 1}^{N} l\left(\mathbf{X}_{i j}, \mathbf{Y}_{i j}\right) \mathbf{s}_{i} \mathbf{s}_{j}
        $$
        $$
          \text{s.t.}\quad \mathbf{s} \in \{0, 1\}^{L} \:\:\text{and}\:\: \mathbf{s}^T \mathbf{1} = N.
        $$
        We evaluate the models on ImageNet-100 and CIFAR-100. We observe that <b>all models have high accuracy for small problem sizes</b>, and that performance drops for larger problem sizes. Thus, some classes are represented similarly and other classes disrupt the matching leading to the steep drop in accuracy. In general, CLIP performs better than DeiT and DINOv2 and on CIFAR-100, DINOv2 performs much worse than the other models. We suspect that some fine-grained classes are represented differently by vision and language models, but explicit language supervision during training may help to align them.
      </p>
      <div class="columns is-centered">
        <div class="column content">
          <img src="static/images/large_scale_imagenet.png" alt="small-scale" width="100%"/>
        </div>
        <div class="column content">
          <img src="static/images/large_scale_cifar.png" alt="small-scale" width="100%"/>
        </div>
      </div>
    </div>
  </div>
</section>
<!--/ End experiments. -->

<!-- Unsupervised classifiers. -->
<section class="section">
  <div class="container is-max-desktop">
  <h2 class="title is-3">Application: Unsupervised classifiers</h2>
    <p class="content has-text-justified">
      Using our matching algorithm, we can find pairings between a set of vision and language embeddings that represent the same objects. We can combine this with any clustering algorithm to build unsupervised classifiers.
      <br>
      Given a classification dataset, we have a set of $N$ classes and for each class a couple of images. We apply a language model to embed each of the classes and cluster the images into $N$ clusters. Finally, we use our matching algorithm to find the correspondence between cluster centers and language embeddings. This returns an assignment of clusters to classes which can be propagated back to each sample in that class.
      <br>
      We achieve non-trivial accuracies for all vision and language models considered and a top accuracy of 51.5% for DINOv2 with All-Roberta-large-v1. While this is far from the supervised state-of-the-art, <b>this is the first instance of fully unsupervised image classification</b>.
    </p>
    <video autoplay muted controls loop playsinline width="100%">
      <source src="./static/videos/unsupervised_classification.mov"
              type="video/mp4">
      <source src="./static/videos/unsupervised_classification.mp4"
              type="video/mp4">
    </video>
  </div>
</section>
<!--/ End unsupervised classifiers. -->

<!-- Discussion -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Discussion</h2>
    <div class="content has-text-justified">
      <h2 class="title is-4">Can we match vision and language without paired data?</h2>
      <ul>
        <li>Many classes can be matched without supervision.</li>
        <li>Some classes have different representations in vision and language, hence cannot be matched.</li>
      </ul>
      <h2 class="title is-4">Can we match arbitrary embeddings?</h2>
      <ul>
        <li>Not every embedding can be matched with existing models.</li>
        <li>Some concepts appear only in one modality, e.g. “freedom of speech”. Thus, explicit supervision is required.</li>
        <li>The blind matching does not scale to very large problems yet ($N \gg 100$).</li>
        <!--N >> 100-->
      </ul>
      <h2 class="title is-4">Which vision model can be matched best?</h2>
      <ul>
        <li>DINOv2 is better on broad concepts.</li>
        <li>CLIP and DeiT encode fine-grained objects more consistently.</li>
      </ul>
    </div>
  </div>
</section>
<!-- End discussion -->

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{schnaus2025it,
        title={It’s a (Blind) Match! Towards Vision-Language Correspondence without Parallel Data},
        author={Schnaus, Dominik and Araslanov, Nikita and Cremers, Daniel},
        booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
        year={2025}
      }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
